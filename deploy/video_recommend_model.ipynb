{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1405e226-457b-41b9-a207-31a9d4d6bdd2",
   "metadata": {},
   "source": [
    "# Video Recommend Model\n",
    "## 1. 모델 훈련\n",
    "konlpy의 morpheme tokenizer와 tfidf vectorizer를 사용하여 콘텐츠 기반 필터링 모델을 구현합니다.\n",
    "- __init__ : vectorizer & tokenizer 객체 초기화\n",
    "- tokenize : 주어진 문장을 형태소 단위의 토큰으로 나누어 리스트에 저장하는 함수\n",
    "- vectorize : 토큰화된 문장을 리스트에 저장하는 함수 (vectorization의 input으로 들어갈 리스트 생성)\n",
    "- fit_transform : 주어진 csv 데이터를 통해 tfidf matrix와 cosine_sim matrix를 생성하는 함수\n",
    "- transform : 새로운 비디오에 대한 cosine_sim matrix를 반환하는 함수\n",
    "- predict : 특정 비디오들과 유사한 비디오 리스트를 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "08a6eb7e-8bb6-4b93-9778-2fbce4125a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================== Import Packages ====================#\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0ae044e0-a4b8-49d0-a051-ccd98137bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class content_based_filtering:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "        self.okt = Okt()\n",
    "        self.tfidf_matrix = None\n",
    "        self.cosine_sim = None\n",
    "        self.videos_df = None\n",
    "\n",
    "    def tokenize(self, contents):\n",
    "        contents_tokens = [self.okt.morphs(row) for row in contents]\n",
    "        return contents_tokens\n",
    "\n",
    "    def vectorize(self, contents_tokens):\n",
    "        contents_vecs = []\n",
    "        for tokens in contents_tokens:\n",
    "            sentence = \"\"\n",
    "            for token in tokens:\n",
    "                sentence += ' ' + token\n",
    "            contents_vecs.append(sentence)\n",
    "        return contents_vecs\n",
    "\n",
    "    def fit_transform(self, filename):\n",
    "        videos_df = pd.read_csv(filename)\n",
    "        videos_df[\"title\"] = videos_df[\"title\"].fillna(\"\")\n",
    "        videos_df[\"description\"] = videos_df[\"description\"].fillna(\"\")\n",
    "        videos_df = videos_df[videos_df['title'] != 'Private video']\n",
    "        videos_df['text'] = videos_df[\"title\"] + \" \" + videos_df[\"description\"]\n",
    "        self.videos_df = videos_df[[\"text\", \"videoId\", \"title\"]]\n",
    "\n",
    "        contents_tokens = self.tokenize(self.videos_df[\"text\"])\n",
    "        contents_vecs = self.vectorize(contents_tokens)\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(contents_vecs)\n",
    "        self.cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "    def transform(self, input_videos):\n",
    "        contents_tokens = self.tokenize(self.videos_df[\"text\"])\n",
    "        contents_vecs = self.vectorize(contents_tokens)\n",
    "        input_tfidf_matrix = self.tfidf_vectorizer.transform(contents_vecs)\n",
    "        cosine_sim = linear_kernel(input_tfidf_matrix, input_tfidf_matrix)\n",
    "        return cosine_sim\n",
    "\n",
    "    def predict(self, video_ids):\n",
    "\n",
    "        final_video_indices = []\n",
    "        for video_id in video_ids:\n",
    "\n",
    "            try:\n",
    "                idx = self.videos_df[self.videos_df['videoId'] == video_id].index[0]\n",
    "\n",
    "            except IndexError:\n",
    "                print(\"null\")\n",
    "                return None\n",
    "\n",
    "            # 해당 영상에 대한 유사도 측정\n",
    "            sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
    "\n",
    "            # 유사도에 따라 정렬\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # 상위 10개 영상 선택\n",
    "            sim_scores = sim_scores[1:11]\n",
    "\n",
    "            # 선택된 영상의 인덱스\n",
    "            final_video_indices += [i[0] for i in sim_scores]\n",
    "\n",
    "        # 선택된 영상의 제목으로 반환\n",
    "        final_video_indices = list(set(final_video_indices))[:10]\n",
    "        return self.videos_df['title'].iloc[final_video_indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7aa9950d-15a2-4329-a9f5-230f1dee305d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['서건창의 날 (240403)', '네일 10승! 기아 VS 키움 8월 13일 하이라이트', '구단 역사상 최다 매진! 스타우트 첫 승!｜ 9월 7일 KIA vs 키움 하이라이트', '2024 한국시리즈 우승콜', '황동하 데뷔 첫 승 경기! 3연승!❤️ | 5월 18일 덕관 | 기아 vs NC', '역전은 백투백이 제맛', '올해 첫 만루홈런의 주인공', '하루 2경기 전승! 🔥｜한국시리즈 2차전 하이라이트', '5연승 질주! 8월 21일 KIA vs 롯데 경기 하이라이트', '14일 SSG전 역전 순간!']\n",
      "[[1.         0.05985762 0.11879739 ... 0.         0.01767005 0.        ]\n",
      " [0.05985762 1.         0.1580122  ... 0.         0.         0.        ]\n",
      " [0.11879739 0.1580122  1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.04017336 0.08090779]\n",
      " [0.01767005 0.         0.         ... 0.04017336 1.         0.1775868 ]\n",
      " [0.         0.         0.         ... 0.08090779 0.1775868  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cbf=content_based_filtering()\n",
    "cbf.fit_transform(\"./kbo_video.csv\")\n",
    "result = cbf.predict([\"AfPaDw2upuk\", \"Dru7TvHX7lY\"])\n",
    "print(result)\n",
    "print(cbf.cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0be8f-ab4e-4e88-a107-bf6b698bbbe9",
   "metadata": {},
   "source": [
    "## 2. S3 모델 업로드\n",
    "훈련된 콘텐츠 기반 필터링 모델을 S3에 pkl 파일로 업로드합니다.\n",
    "- video_df.pkl : 야구 영상 데이터프레임을 직렬화한 pkl 파일\n",
    "- cosine_sim.pkl : 야구 영상에 관한 코사인 유사도 행렬을 직렬화한 pkl 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "15adc3a2-651a-43cf-9d40-e80598719436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "import boto3\n",
    "import shutil\n",
    "import joblib\n",
    "import logging\n",
    "import sagemaker\n",
    "\n",
    "# S3 클라이언트 설정\n",
    "region = 'ap-northeast-2'\n",
    "#boto3.setup_default_session(region_name=region)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# SageMaker 역할 및 세션 설정\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "def upload_model_to_s3(model, bucket_name, model_filename):\n",
    "\n",
    "    # 임시 디렉토리 생성\n",
    "    model_dir = \"models\"  # 상대 경로로 설정\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 모델 직렬화 (pkl 파일로 저장)\n",
    "    #model_pkl_path = os.path.join(model_dir, \"content_based_model.pkl\")\n",
    "    model_pkl_path = os.path.join(model_dir, \"video_df.pkl\")\n",
    "    with open(model_pkl_path, \"wb\") as model_file:\n",
    "        #model_to_save = model.video_df\n",
    "        model.videos_df.to_pickle(model_file)\n",
    "    \n",
    "    model_pkl_path = os.path.join(model_dir, \"cosine_sim.pkl\")\n",
    "    with open(model_pkl_path, \"wb\") as model_file:\n",
    "        model_to_save = model.cosine_sim.tolist()\n",
    "        joblib.dump(model_to_save, model_file)\n",
    "\n",
    "    # predictor.py를 모델 디렉토리에 복사\n",
    "    predictor_script_path = \"predictor.py\"\n",
    "    shutil.copy(predictor_script_path, model_dir)\n",
    "\n",
    "    # tar.gz 형식으로 압축\n",
    "    tar_gz_path = model_filename  # 압축할 최종 파일 경로\n",
    "    with tarfile.open(tar_gz_path, 'w:gz') as tar:\n",
    "        #tar.add('models/content_based_model.pkl', arcname='content_based_model.pkl')\n",
    "        tar.add(\"models/video_df.pkl\", arcname=\"video_df.pkl\")\n",
    "        tar.add(\"models/cosine_sim.pkl\", arcname=\"cosine_sim.pkl\")\n",
    "\n",
    "    # S3에 압축된 모델 파일 업로드\n",
    "    s3_path = f\"models/{os.path.basename(tar_gz_path)}\"\n",
    "    s3_client.upload_file(tar_gz_path, bucket_name, s3_path)\n",
    "\n",
    "#bucket_name = sagemaker_session.default_bucket()\n",
    "bucket_name = \"yaong-baseball\"\n",
    "model_name = \"models/content_based_model.tar.gz\"\n",
    "\n",
    "upload_model_to_s3(cbf, bucket_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "83835108-a1ec-44f1-a72a-1ecb0e543744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists in the specified S3 path\n",
    "try:\n",
    "    s3_client.head_object(Bucket=bucket_name, Key=model_name)\n",
    "    print(\"File exists!\")\n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    print(\"File not found:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3035b-1acc-4177-b692-7b48046ab608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
