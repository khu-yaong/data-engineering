{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1405e226-457b-41b9-a207-31a9d4d6bdd2",
   "metadata": {},
   "source": [
    "# Video Recommend Model\n",
    "## 1. ëª¨ë¸ í›ˆë ¨\n",
    "konlpyì˜ morpheme tokenizerì™€ tfidf vectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "- __init__ : vectorizer & tokenizer ê°ì²´ ì´ˆê¸°í™”\n",
    "- tokenize : ì£¼ì–´ì§„ ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í°ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "- vectorize : í† í°í™”ëœ ë¬¸ì¥ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (vectorizationì˜ inputìœ¼ë¡œ ë“¤ì–´ê°ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±)\n",
    "- fit_transform : ì£¼ì–´ì§„ csv ë°ì´í„°ë¥¼ í†µí•´ tfidf matrixì™€ cosine_sim matrixë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "- transform : ìƒˆë¡œìš´ ë¹„ë””ì˜¤ì— ëŒ€í•œ cosine_sim matrixë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "- predict : íŠ¹ì • ë¹„ë””ì˜¤ë“¤ê³¼ ìœ ì‚¬í•œ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "08a6eb7e-8bb6-4b93-9778-2fbce4125a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================== Import Packages ====================#\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0ae044e0-a4b8-49d0-a051-ccd98137bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class content_based_filtering:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "        self.okt = Okt()\n",
    "        self.tfidf_matrix = None\n",
    "        self.cosine_sim = None\n",
    "        self.videos_df = None\n",
    "\n",
    "    def tokenize(self, contents):\n",
    "        contents_tokens = [self.okt.morphs(row) for row in contents]\n",
    "        return contents_tokens\n",
    "\n",
    "    def vectorize(self, contents_tokens):\n",
    "        contents_vecs = []\n",
    "        for tokens in contents_tokens:\n",
    "            sentence = \"\"\n",
    "            for token in tokens:\n",
    "                sentence += ' ' + token\n",
    "            contents_vecs.append(sentence)\n",
    "        return contents_vecs\n",
    "\n",
    "    def fit_transform(self, filename):\n",
    "        videos_df = pd.read_csv(filename)\n",
    "        videos_df[\"title\"] = videos_df[\"title\"].fillna(\"\")\n",
    "        videos_df[\"description\"] = videos_df[\"description\"].fillna(\"\")\n",
    "        videos_df = videos_df[videos_df['title'] != 'Private video']\n",
    "        videos_df['text'] = videos_df[\"title\"] + \" \" + videos_df[\"description\"]\n",
    "        self.videos_df = videos_df[[\"text\", \"videoId\", \"title\"]]\n",
    "\n",
    "        contents_tokens = self.tokenize(self.videos_df[\"text\"])\n",
    "        contents_vecs = self.vectorize(contents_tokens)\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(contents_vecs)\n",
    "        self.cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "    def transform(self, input_videos):\n",
    "        contents_tokens = self.tokenize(self.videos_df[\"text\"])\n",
    "        contents_vecs = self.vectorize(contents_tokens)\n",
    "        input_tfidf_matrix = self.tfidf_vectorizer.transform(contents_vecs)\n",
    "        cosine_sim = linear_kernel(input_tfidf_matrix, input_tfidf_matrix)\n",
    "        return cosine_sim\n",
    "\n",
    "    def predict(self, video_ids):\n",
    "\n",
    "        final_video_indices = []\n",
    "        for video_id in video_ids:\n",
    "\n",
    "            try:\n",
    "                idx = self.videos_df[self.videos_df['videoId'] == video_id].index[0]\n",
    "\n",
    "            except IndexError:\n",
    "                print(\"null\")\n",
    "                return None\n",
    "\n",
    "            # í•´ë‹¹ ì˜ìƒì— ëŒ€í•œ ìœ ì‚¬ë„ ì¸¡ì •\n",
    "            sim_scores = list(enumerate(self.cosine_sim[idx]))\n",
    "\n",
    "            # ìœ ì‚¬ë„ì— ë”°ë¼ ì •ë ¬\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # ìƒìœ„ 10ê°œ ì˜ìƒ ì„ íƒ\n",
    "            sim_scores = sim_scores[1:11]\n",
    "\n",
    "            # ì„ íƒëœ ì˜ìƒì˜ ì¸ë±ìŠ¤\n",
    "            final_video_indices += [i[0] for i in sim_scores]\n",
    "\n",
    "        # ì„ íƒëœ ì˜ìƒì˜ ì œëª©ìœ¼ë¡œ ë°˜í™˜\n",
    "        final_video_indices = list(set(final_video_indices))[:10]\n",
    "        return self.videos_df['title'].iloc[final_video_indices].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7aa9950d-15a2-4329-a9f5-230f1dee305d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì„œê±´ì°½ì˜ ë‚  (240403)', 'ë„¤ì¼ 10ìŠ¹! ê¸°ì•„ VS í‚¤ì›€ 8ì›” 13ì¼ í•˜ì´ë¼ì´íŠ¸', 'êµ¬ë‹¨ ì—­ì‚¬ìƒ ìµœë‹¤ ë§¤ì§„! ìŠ¤íƒ€ìš°íŠ¸ ì²« ìŠ¹!ï½œ 9ì›” 7ì¼ KIA vs í‚¤ì›€ í•˜ì´ë¼ì´íŠ¸', '2024 í•œêµ­ì‹œë¦¬ì¦ˆ ìš°ìŠ¹ì½œ', 'í™©ë™í•˜ ë°ë·” ì²« ìŠ¹ ê²½ê¸°! 3ì—°ìŠ¹!â¤ï¸ | 5ì›” 18ì¼ ë•ê´€ | ê¸°ì•„ vs NC', 'ì—­ì „ì€ ë°±íˆ¬ë°±ì´ ì œë§›', 'ì˜¬í•´ ì²« ë§Œë£¨í™ˆëŸ°ì˜ ì£¼ì¸ê³µ', 'í•˜ë£¨ 2ê²½ê¸° ì „ìŠ¹! ğŸ”¥ï½œí•œêµ­ì‹œë¦¬ì¦ˆ 2ì°¨ì „ í•˜ì´ë¼ì´íŠ¸', '5ì—°ìŠ¹ ì§ˆì£¼! 8ì›” 21ì¼ KIA vs ë¡¯ë° ê²½ê¸° í•˜ì´ë¼ì´íŠ¸', '14ì¼ SSGì „ ì—­ì „ ìˆœê°„!']\n",
      "[[1.         0.05985762 0.11879739 ... 0.         0.01767005 0.        ]\n",
      " [0.05985762 1.         0.1580122  ... 0.         0.         0.        ]\n",
      " [0.11879739 0.1580122  1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.04017336 0.08090779]\n",
      " [0.01767005 0.         0.         ... 0.04017336 1.         0.1775868 ]\n",
      " [0.         0.         0.         ... 0.08090779 0.1775868  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cbf=content_based_filtering()\n",
    "cbf.fit_transform(\"./kbo_video.csv\")\n",
    "result = cbf.predict([\"AfPaDw2upuk\", \"Dru7TvHX7lY\"])\n",
    "print(result)\n",
    "print(cbf.cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0be8f-ab4e-4e88-a107-bf6b698bbbe9",
   "metadata": {},
   "source": [
    "## 2. S3 ëª¨ë¸ ì—…ë¡œë“œ\n",
    "í›ˆë ¨ëœ ì½˜í…ì¸  ê¸°ë°˜ í•„í„°ë§ ëª¨ë¸ì„ S3ì— pkl íŒŒì¼ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "- video_df.pkl : ì•¼êµ¬ ì˜ìƒ ë°ì´í„°í”„ë ˆì„ì„ ì§ë ¬í™”í•œ pkl íŒŒì¼\n",
    "- cosine_sim.pkl : ì•¼êµ¬ ì˜ìƒì— ê´€í•œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ì„ ì§ë ¬í™”í•œ pkl íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "15adc3a2-651a-43cf-9d40-e80598719436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "import boto3\n",
    "import shutil\n",
    "import joblib\n",
    "import logging\n",
    "import sagemaker\n",
    "\n",
    "# S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "region = 'ap-northeast-2'\n",
    "#boto3.setup_default_session(region_name=region)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# SageMaker ì—­í•  ë° ì„¸ì…˜ ì„¤ì •\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "def upload_model_to_s3(model, bucket_name, model_filename):\n",
    "\n",
    "    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    model_dir = \"models\"  # ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì •\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # ëª¨ë¸ ì§ë ¬í™” (pkl íŒŒì¼ë¡œ ì €ì¥)\n",
    "    #model_pkl_path = os.path.join(model_dir, \"content_based_model.pkl\")\n",
    "    model_pkl_path = os.path.join(model_dir, \"video_df.pkl\")\n",
    "    with open(model_pkl_path, \"wb\") as model_file:\n",
    "        #model_to_save = model.video_df\n",
    "        model.videos_df.to_pickle(model_file)\n",
    "    \n",
    "    model_pkl_path = os.path.join(model_dir, \"cosine_sim.pkl\")\n",
    "    with open(model_pkl_path, \"wb\") as model_file:\n",
    "        model_to_save = model.cosine_sim.tolist()\n",
    "        joblib.dump(model_to_save, model_file)\n",
    "\n",
    "    # predictor.pyë¥¼ ëª¨ë¸ ë””ë ‰í† ë¦¬ì— ë³µì‚¬\n",
    "    predictor_script_path = \"predictor.py\"\n",
    "    shutil.copy(predictor_script_path, model_dir)\n",
    "\n",
    "    # tar.gz í˜•ì‹ìœ¼ë¡œ ì••ì¶•\n",
    "    tar_gz_path = model_filename  # ì••ì¶•í•  ìµœì¢… íŒŒì¼ ê²½ë¡œ\n",
    "    with tarfile.open(tar_gz_path, 'w:gz') as tar:\n",
    "        #tar.add('models/content_based_model.pkl', arcname='content_based_model.pkl')\n",
    "        tar.add(\"models/video_df.pkl\", arcname=\"video_df.pkl\")\n",
    "        tar.add(\"models/cosine_sim.pkl\", arcname=\"cosine_sim.pkl\")\n",
    "\n",
    "    # S3ì— ì••ì¶•ëœ ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ\n",
    "    s3_path = f\"models/{os.path.basename(tar_gz_path)}\"\n",
    "    s3_client.upload_file(tar_gz_path, bucket_name, s3_path)\n",
    "\n",
    "#bucket_name = sagemaker_session.default_bucket()\n",
    "bucket_name = \"yaong-baseball\"\n",
    "model_name = \"models/content_based_model.tar.gz\"\n",
    "\n",
    "upload_model_to_s3(cbf, bucket_name, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "83835108-a1ec-44f1-a72a-1ecb0e543744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "# Check if the file exists in the specified S3 path\n",
    "try:\n",
    "    s3_client.head_object(Bucket=bucket_name, Key=model_name)\n",
    "    print(\"File exists!\")\n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    print(\"File not found:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3035b-1acc-4177-b692-7b48046ab608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
